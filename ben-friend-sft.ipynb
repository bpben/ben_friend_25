{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bpben/ben_friend_25/blob/main/ben-friend-sft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZckM4owUm_8w"
      },
      "source": [
        "# Ben Needs a Friend Supervised Fine-Tuning (SFT) with Unsloth\n",
        "This notebook demonstrates an approach to fine-tuning a Llama 3 model via SFT.\n",
        "\n",
        "We're using [Unsloth](https://unsloth.ai/) to make this process more efficient and able to fit on a pretty small GPU (NVIDIA T4).\n",
        "\n",
        "I adapted this from [Unsloth's tutorial materials](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2siOz5Nm_8x"
      },
      "outputs": [],
      "source": [
        "# required configuration for Colab environment\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.14.0 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    # needed for loading the dataset\n",
        "    !pip install -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdo7PPimJSZB"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOvz_LM_InQv"
      },
      "source": [
        "Here we initialize the model as a `FastLanguageModel`.  This is Unsloth's optimized version of a language model and allows us to do faster inference and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GAEGqhPm_8x"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # 4bit quantization to reduce memory usage\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "Here we attach a Low Rank Adapter (LoRA) for parameter-efficient fine-tuning.  We target a specific set of layers in the model:\n",
        "\n",
        "`q/k/v_proj` - Has to do with the [attention mechanism](https://jalammar.github.io/illustrated-transformer/) of the transformer architecture.\n",
        "\n",
        "`o_proj` - Handles conversion from attention module back into the rest of the model\n",
        "\n",
        "'gate/up/down_proj` - FFNN components (see transformer diagram)\n",
        "\n",
        "\n",
        "These are some standard parameters, but can be adjusted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # governs the rank of the LoRA matrix\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # dropout applied to the LoRA matrix, 0 is optimized in unsloth\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "Here I used a dataset I previously processed that has pairs of exchanges between the major characters in Friends.  I've arranged it in the following format:\n",
        "\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```\n",
        "\n",
        "We need to convert it to the fine-tuning template for Llama 3.1:\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What is 2+2?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "It's 4.<|eot_id|>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
        "from datasets import load_dataset, Dataset, interleave_datasets\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    # utility to apply the chat template to the conversations\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# pulls from huggingface\n",
        "# script dataset is product of another notebook\n",
        "dataset = load_dataset(\"bpben/friends_script\", split='train')\n",
        "print(\"\\nRaw\")\n",
        "display(dataset[0]['conversations'])\n",
        "\n",
        "# apply template\n",
        "print(\"\\nFine-tuning formatted\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "display(dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to mix into this conversation data some standard instruction tuning data.  The idea here is to avoid [catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference).  I want it to still be helpful, even if the text it's trained on isn't...quite."
      ],
      "metadata": {
        "id": "8r-dTm-Zmo5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# want to additionally interleave data from an actual instruction tuning dataset\n",
        "def formatting_inst_data(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "inst_dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")\n",
        "inst_dataset = standardize_sharegpt(inst_dataset)\n",
        "inst_dataset = inst_dataset.map(formatting_prompts_func, batched = True,)\n",
        "combined_dataset = interleave_datasets([dataset, inst_dataset], seed = 3407)\n",
        "combined_dataset[1]['text']"
      ],
      "metadata": {
        "id": "gFJSa5xZ78q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfzTdMtvGE6w"
      },
      "source": [
        "Here I want to run inference on the same example from the Unsloth tutorial.  This will give us a baseline and we will see how the model changes after training.\n",
        "\n",
        "Interestingly the tutorial uses `min_p = 0.1` and `temperature = 1.5`, as it seems like better results [have been observed](https://x.com/menhguin/status/1826132708508213629) with these parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHh9m5k24mwd"
      },
      "outputs": [],
      "source": [
        "# testing out its attitude\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What are you up to tonight?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
        "                         temperature = 1.5, min_p = 0.1)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's set up the trainer ([TRL SFT](https://huggingface.co/docs/trl/sft_trainer)).  This will take a bit as it collates all the information.\n",
        "\n",
        "We're just going to train on a small subset of these data, and you will see it already breaks the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = combined_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # running this just for a while, can edit accordingly\n",
        "        max_steps = 500,\n",
        "        #num_train_epochs=1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 50,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juQiExuBG5Bt"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "To verify if the masking is actually done, we should only see \"labels\" attached to tokens from the assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtdHhskp0v_h"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rD6fl8EUxnG"
      },
      "outputs": [],
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enWUM0jV-jV"
      },
      "source": [
        "We can see the System and Instruction prompts are successfully masked!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What are you up to tonight?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
        "                         temperature = 1.5, min_p = 0.1)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load and predict\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "model_id = \"bpben/ben_friend_sft\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_id,\n",
        "    load_in_4bit = True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "OrKi8wr8jW2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Whoa what a night!\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
        "                         temperature = 1.5, min_p = 0.1)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "UBUzz4xP5uXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A note on usage with Ollama\n",
        "Ollama (as of this writing) does not appear to accept the safetensors format for adapters, or at least not the format that unsloth pushes to the hub.\n",
        "\n",
        "As a result, you can either use unsloth's own way to export the entire model: `model.save_pretrained_gguf('sft_friend', tokenizer)`\n",
        "\n",
        "Or you can follow the steps here: https://sarinsuriyakoon.medium.com/unsloth-lora-with-ollama-lightweight-solution-to-full-cycle-llm-development-edadb6d9e0f0\n",
        "\n",
        "For the tutorial example, I used the second option as I just wanted the adapter, not the entire model."
      ],
      "metadata": {
        "id": "xg6vHWM601yF"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}