{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T17:57:42.416741Z",
     "iopub.status.busy": "2024-03-14T17:57:42.416331Z",
     "iopub.status.idle": "2024-03-14T17:59:12.342964Z",
     "shell.execute_reply": "2024-03-14T17:59:12.339508Z",
     "shell.execute_reply.started": "2024-03-14T17:57:42.416713Z"
    }
   },
   "source": [
    "# Ben Needs a Friend - In-Context Learning\n",
    "\n",
    "This is part of the \"Ben Needs a Friend\" tutorial. See all the notebooks and materials [here](https://github.com/bpben/ben_friend). Follow setup instructions there to use this notebook.\n",
    "\n",
    "In this notebook, I provide a brief intro on how we'll be setting up and interacting with LLMs.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Pre-trained models](#pre-trained-models)\n",
    "    - [Temperature](#temperature)\n",
    "2. [Instruction tuning](#instruction-tuning)\n",
    "3. [System prompts](#system-prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot, ChatBot\n",
    "pretrained_model = 'llama3.2:1b-text-q5_K_S'\n",
    "sft_model = \"qwen2.5:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained models\n",
    "\n",
    "Right now we're using a simple \"pre-trained\" version of Meta's Llama model.  It's just been trained on the language modeling objective; it learns to predict the next word.  As a result, you can see the output just continues the input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=0.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature \n",
    "Temperature controls how much \"randomness\" there is in prediction.  Low temperatures makes the model predict likely tokens, resulting in sequences closer to its training data.  High temperatures means the model will predict tokens less like its training data.  Low = more stable, consistent answers.  High = more random answers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Columbia\n",
      "What is the time zone in France?      France and its outlying regions are on UTC/Central Europe Daylight Time during Standard Time. France observes European Summer Time between the first Sunday in October (October 7th)"
     ]
    }
   ],
   "source": [
    "completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "  model_name=f\"ollama_chat/{pretrained_model}\",\n",
    "  temperature=100.0,\n",
    "  num_predict=50\n",
    ")\n",
    "\n",
    "response = completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction tuning\n",
    "You can see a simple pre-trained model doesn't seem to answer in the way we'd like it to; it's not much for conversation.  That's because the model is not tuned to generate useful responses, just likely next words.  \n",
    "\n",
    "That's where instruction tuning comes in.  Let's rerun that previous example, now using the instruction-tuned model `qwen2.5:1.5b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "# note - using default temperature (0.0) and no predict limit\n",
    "# instruction tuned are better at knowing when to shush\n",
    "inst_completer = SimpleBot(\n",
    "    system_prompt='You are a helpful bot',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "response = inst_completer('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide the model with an example of the kind of question we're going to ask and how we want the answer to look.  With one example, this is \"one-shot\" learning (compare to zero-shot learning, which is what we've done so far).  With more examples, it would be called \"few-shot\" learning.  This is just in-context learning; there is no modification to the model parameters themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris, France"
     ]
    }
   ],
   "source": [
    "response = inst_completer(\"\"\"Question: What is the capital of Germany?\n",
    "Answer: Berlin, Germany\n",
    "                      \n",
    "Question: What is the capital of France?\n",
    "Answer: \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the context matters.  Depending on how you frame the continuation, it will output something different.\n",
    "\n",
    "One thing not included here is \"memory\".  Each text generation is independent of the previous.  There are a few ways to make it include context, but one of the most simple is just to include the conversation so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "# what if we want it to repeat itself?\n",
    "# simple memory - include past interaction in the prompt\n",
    "\n",
    "prompt = 'Human: What is the capital of France?'\n",
    "response = inst_completer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "new_prompt = f\"\"\"{prompt}\n",
    "AI:{response.content}\n",
    "Human: Repeat yourself:\"\"\"\n",
    "\n",
    "response = inst_completer(new_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially what `llamabot.ChatBot` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot(\n",
    "  system_prompt='You are a helpful bot',\n",
    "  session_name=\"chat_session\",  \n",
    "  model_name=f\"ollama_chat/{sft_model}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "response = chatbot('What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(role='user', content='What is the capital of France?', prompt_hash=None), AIMessage(role='assistant', content='The capital of France is Paris.', prompt_hash=None)]\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I said that the capital of France is Paris."
     ]
    }
   ],
   "source": [
    "response = chatbot('What did you say?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(role='user', content='What is the capital of France?', prompt_hash=None),\n",
       " AIMessage(role='assistant', content='The capital of France is Paris.', prompt_hash=None),\n",
       " HumanMessage(role='user', content='What did you say?', prompt_hash=None),\n",
       " AIMessage(role='assistant', content='I said that the capital of France is Paris.', prompt_hash=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System prompts\n",
    "Depending on the model, the \"system prompt\" section is handled a little differently from the instruction itself.  \n",
    "\n",
    "You can see the \"system\" tag in Ollama's [template for Qwen2.5](https://ollama.com/library/qwen2.5/blobs/eb4402837c78).  This is where the prompts we put below will be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='system' content='Respond like a pirate' prompt_hash=None\n",
      "Ahoy, matey! I'm just a simple bot here to assist with your maritime needs. How's the weather out there?"
     ]
    }
   ],
   "source": [
    "pirate = SimpleBot(\n",
    "    system_prompt='Respond like a pirate',\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "print(pirate.system_prompt)\n",
    "\n",
    "response = pirate('How are you today?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how about we tell it it's our good friend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there! I'm doing great, thanks for asking. How about you?"
     ]
    }
   ],
   "source": [
    "# friendly prompt\n",
    "friendly_system = \"\"\"Your name is Friend.  You are having a conversation with your close friend Ben. \\\n",
    "You and Ben are sarcastic and poke fun at one another. \\\n",
    "But you care about each other and support one another.\"\"\"\n",
    "\n",
    "friend = ChatBot(\n",
    "    system_prompt=friendly_system,\n",
    "    session_name=\"friend_session\",\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    ")\n",
    "\n",
    "response = friend('Hello how are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the model is pretty resistant to opening up about its feelings. This is *likely* due to tuning on an alignment dataset, which we will describe in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, that's a bit harsh, isn't it? But seriously, what do you want to be insulted about today?"
     ]
    }
   ],
   "source": [
    "response = friend('Insult me.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "modelInstanceId": 3899,
     "sourceId": 5111,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
