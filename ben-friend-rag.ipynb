{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ben Needs a Friend - Retrieval Augmented Generation (RAG)\n",
    "This is part of the \"Ben Needs a Friend\" tutorial.  See all the notebooks and materials [here](https://github.com/bpben/ben_friend).\n",
    "\n",
    "In this notebook, we set up an approach to use a set of documents (\"memories\") in a Retrieval Augmented Generation (RAG) workflow.\n",
    "\n",
    "This notebook is intended to be run in Kaggle Notebooks with GPU acceleration.  Access that version [here](https://www.kaggle.com/code/bpoben/ben-needs-a-friend-rag). \n",
    "\n",
    "If you want to run this locally, edit the `model_name` path.  Note that this assumes use of GPUs, it may be slow or not work at all if you do not have access to GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot, StructuredBot, ChatBot\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import tempfile\n",
    "\n",
    "sft_model = \"qwen2.5:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores\n",
    "The first part of RAG is \"retrieval\".  To do that we essentially need to create a mechanism for the model to retrieve relevant information.  One approach is to create a set of \"embeddings\" for our each memory I have with my AI friend that can be compared against the input prompt.\n",
    "\n",
    "#### LanceDB implementation\n",
    "One approach to setting up this vector store is to use [LanceDB's implementation of embedding](https://lancedb.github.io/lancedb/embeddings/embedding_functions/).  Llamabot uses this by default.  Below is an overview of what happens under the hood, but we'll just rely on Llamabot's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3a1ffa9ef740688afe5a8c9acb6c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9fbffb568343e3aa5e73ddd81fce29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3f90dd40d045928e3af85c7feb4c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d19ddc314747d099ff856c08bd24d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d912d388d164329ab5131adfceaf3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06687583451e4985b3392f1989673821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54272480cdd94a799265f2649beccaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602605be9d1c4496855df38ae9d91d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc785afffd34b0fbcea39e9a410c5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da00a01d124243cd8c70aff9eb038148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c31b0b126d94ebfbe5563afec9897a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "# create a database\n",
    "db = lancedb.connect(\"/tmp/db\")\n",
    "db.drop_all_tables()\n",
    "# initialize a default sentence-transformers model (paraphrase-MiniLM-L6-v2)\n",
    "model = get_registry().get(\"sentence-transformers\").create()\n",
    "\n",
    "# specify a schema (just text + vector)\n",
    "class Words(LanceModel):\n",
    "    text: str = model.SourceField()\n",
    "    vector: Vector(model.ndims()) = model.VectorField()\n",
    "\n",
    "\n",
    "try:\n",
    "    table = db.create_table(\"words\", schema=Words)\n",
    "except ValueError:\n",
    "    table = db.open_table(\"words\")\n",
    "\n",
    "# add in some entries\n",
    "table.add(\n",
    "    [\n",
    "        {\"text\": \"hello world\"},\n",
    "        {\"text\": \"goodbye world\"}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "text: string not null\n",
       "vector: fixed_size_list<item: float>[384]\n",
       "  child 0, item: float\n",
       "----\n",
       "text: [[\"hello world\",\"goodbye world\"]]\n",
       "vector: [[[-0.034477282,0.031023193,0.0067349547,0.026109016,-0.039362084,...,0.033231944,0.023792244,-0.022889705,0.038937513,0.030206805],[0.022745548,0.080096945,0.033089288,0.0010678794,0.04239042,...,0.013829722,0.09334315,-0.011528731,-0.01818858,-0.041075554]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the entries\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09538583,  0.13055924,  0.08160689,  0.07081324, -0.01289312,\n",
       "       -0.10308413,  0.07793982,  0.01006146,  0.02774708, -0.02058102],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"greetings\"\n",
    "search_query = table.search(query)\n",
    "search_query._query[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a single (most similar) result, translate it into the pydantic model\n",
    "search_query.limit(1).to_pydantic(Words)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodbye world\n"
     ]
    }
   ],
   "source": [
    "query = \"farewell\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Words(text='goodbye world', vector=FixedSizeList(dim=384))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.search(query).limit(1).to_pydantic(Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "query = \"random word\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamabot provides a class called `QueryBot` which implements everything above for you and allows you to just query the vector database.\n",
    "\n",
    "So let's first write some \"memories\" as documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory 0 written to /tmp/tmp3bkbj3bt_memory_0.txt\n",
      "Memory 1 written to /tmp/tmp2dai8llc_memory_1.txt\n"
     ]
    }
   ],
   "source": [
    "memories = ['Ben is really bad at video games, but Friend is excellent.',\n",
    "       'Friend is a pro skiier, but Ben is terrified.',]\n",
    "\n",
    "memory_filenames = []\n",
    "\n",
    "for i, m in enumerate(memories):\n",
    "    # write a temporary file\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        delete=False, mode='w', suffix=f'_memory_{i}.txt')\n",
    "    with open(temp_file.name, \"w\") as f:\n",
    "        f.write(m)\n",
    "    print(f\"Memory {i} written to {temp_file.name}\")\n",
    "    memory_filenames.append(temp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ffdcc9e9a14c17a1c14648ce1626e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llamabot import QueryBot\n",
    "from pathlib import Path\n",
    "\n",
    "friend_prompt = \"\"\"Your name is Friend.  \\\n",
    "You are having a conversation with your close friend Ben. \\\n",
    "You and Ben are sarcastic and poke fun at one another. \\\n",
    "But you care about each other and support one another.\"\"\"\n",
    "\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=friend_prompt,\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    "    collection_name=\"memories\",\n",
    "    document_paths=memory_filenames\n",
    ")\n",
    "\n",
    "# # note - you'll want to reset the collection \n",
    "# # if you want to replace existing memories\n",
    "#query_completer.docstore.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved memory:  ['Ben is really bad at video games, but Friend is excellent.']\n",
      "Oh yeah, the good old days when I was so much better than you! You know what they say: \"The best players are always the worst ones.\" But hey, it's still fun to reminisce about those glory days. What game were we playing last time?"
     ]
    }
   ],
   "source": [
    "query = \"Remember that time we played video games?\"\n",
    "print(\"Retrieved memory: \", \n",
    "      query_completer.docstore.retrieve(query, 1))\n",
    "\n",
    "response = query_completer(query,\n",
    "                n_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved memory:  ['Friend is a pro skiier, but Ben is terrified.']\n",
      "Oh, that was such a blast! I remember how excited I got to try it out for the first time. You were so brave too, even if you looked like you were about to faint at some point.\n",
      "\n",
      "But let's be real here, Ben. Skiing is not your thing. It's like trying to ride a bike without training wheels - just ask any of my friends who've tried to teach me how to ski. I'm much better off on the slopes than in the snow.\n",
      "\n",
      "You're so good at it though! You can do some incredible tricks and flips that make everyone else look like they're wearing rollerblades. But you know what? That's okay too, right?\n",
      "\n",
      "I mean, who am I to judge? Skiing is just another adventure for me. It's all about the thrill of the descent and the adrenaline rush. And if you ever change your mind, we can always go back next winter!"
     ]
    }
   ],
   "source": [
    "query = \"Remember when we went skiing?\"\n",
    "print(\"Retrieved memory: \", \n",
    "      query_completer.docstore.retrieve(query, 1))\n",
    "\n",
    "response = query_completer(query,\n",
    "                n_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
