{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ben Needs a Friend - Retrieval Augmented Generation (RAG)\n",
    "This is part of the \"Ben Needs a Friend\" tutorial. See all the notebooks and materials [here](https://github.com/bpben/ben_friend_25). Follow setup instructions there to use this notebook.\n",
    "\n",
    "In this notebook, we set up an approach to use a set of documents (\"memories\") in a Retrieval Augmented Generation (RAG) workflow.\n",
    "\n",
    "This notebook is intended to be run in Kaggle Notebooks with GPU acceleration.  Access that version [here](https://www.kaggle.com/code/bpoben/ben-needs-a-friend-rag). \n",
    "\n",
    "If you want to run this locally, edit the `model_name` path.  Note that this assumes use of GPUs, it may be slow or not work at all if you do not have access to GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot, StructuredBot, ChatBot\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import tempfile\n",
    "\n",
    "sft_model = \"qwen2.5:1.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores\n",
    "The first part of RAG is \"retrieval\".  To do that we essentially need to create a mechanism for the model to retrieve relevant information.  One approach is to create a set of \"embeddings\" for our each memory I have with my AI friend that can be compared against the input prompt.\n",
    "\n",
    "#### LanceDB implementation\n",
    "One approach to setting up this vector store is to use [LanceDB's implementation of embedding](https://lancedb.github.io/lancedb/embeddings/embedding_functions/).  Llamabot uses this by default.  Below is an overview of what happens under the hood, but we'll just rely on Llamabot's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "# create a database\n",
    "db = lancedb.connect(\"/tmp/db\")\n",
    "db.drop_all_tables()\n",
    "# initialize a default sentence-transformers model (paraphrase-MiniLM-L6-v2)\n",
    "model = get_registry().get(\"sentence-transformers\").create()\n",
    "\n",
    "# specify a schema (just text + vector)\n",
    "class Words(LanceModel):\n",
    "    text: str = model.SourceField()\n",
    "    vector: Vector(model.ndims()) = model.VectorField()\n",
    "\n",
    "\n",
    "try:\n",
    "    table = db.create_table(\"words\", schema=Words)\n",
    "except ValueError:\n",
    "    table = db.open_table(\"words\")\n",
    "\n",
    "# add in some entries\n",
    "table.add(\n",
    "    [\n",
    "        {\"text\": \"hello world\"},\n",
    "        {\"text\": \"goodbye world\"}\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the entries\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"greetings\"\n",
    "search_query = table.search(query)\n",
    "search_query._query[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single (most similar) result, translate it into the pydantic model\n",
    "search_query.limit(1).to_pydantic(Words)[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"farewell\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.search(query).limit(1).to_pydantic(Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"random word\"\n",
    "result = table.search(query).limit(1).to_pydantic(Words)[0]\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamabot provides a class called `QueryBot` which implements everything above for you and allows you to just query the vector database.\n",
    "\n",
    "So let's first write some \"memories\" as documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memories = ['Ben is really bad at video games, but Friend is excellent.',\n",
    "       'Friend is a pro skiier, but Ben is terrified.',]\n",
    "\n",
    "memory_filenames = []\n",
    "\n",
    "for i, m in enumerate(memories):\n",
    "    # write a temporary file\n",
    "    temp_file = tempfile.NamedTemporaryFile(\n",
    "        delete=False, mode='w', suffix=f'_memory_{i}.txt')\n",
    "    with open(temp_file.name, \"w\") as f:\n",
    "        f.write(m)\n",
    "    print(f\"Memory {i} written to {temp_file.name}\")\n",
    "    memory_filenames.append(temp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import QueryBot\n",
    "from pathlib import Path\n",
    "\n",
    "friend_prompt = \"\"\"Your name is Friend.  \\\n",
    "You are having a conversation with your close friend Ben. \\\n",
    "You and Ben are sarcastic and poke fun at one another. \\\n",
    "But you care about each other and support one another.\"\"\"\n",
    "\n",
    "query_completer = QueryBot(\n",
    "    system_prompt=friend_prompt,\n",
    "    model_name=f\"ollama_chat/{sft_model}\",\n",
    "    collection_name=\"memories\",\n",
    "    document_paths=memory_filenames\n",
    ")\n",
    "\n",
    "# # note - you'll want to reset the collection \n",
    "# # if you want to replace existing memories\n",
    "#query_completer.docstore.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Remember that time we played video games?\"\n",
    "print(\"Retrieved memory: \", \n",
    "      query_completer.docstore.retrieve(query, 1))\n",
    "\n",
    "response = query_completer(query,\n",
    "                n_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Remember when we went skiing?\"\n",
    "print(\"Retrieved memory: \", \n",
    "      query_completer.docstore.retrieve(query, 1))\n",
    "\n",
    "response = query_completer(query,\n",
    "                n_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
